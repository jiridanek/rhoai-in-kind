on:
  pull_request:
  push:
  workflow_dispatch:
  schedule:
    - cron: '0 2 * * *' # at 02:00

jobs:
  rhoai-in-kind:
    runs-on: ubuntu-latest

    strategy:
      fail-fast: false
      matrix:
        workbench_branch:
          - rhoai-2.22

        test_case:
          - "**/workbenches/testWorkbenchControlSuite.cy.ts"

          # this checks user perms but all my users are in effect admins
          - "**/workbenches/testWorkbenchCreation.cy.ts"

          - "**/workbenches/testWorkbenchImages.cy.ts"

          # I'm removing resource requests, so even XXL workbench starts up
          #- "**/workbenches/testWorkbenchNegativeTests.cy.ts"

          - "**/workbenches/testWorkbenchStatus.cy.ts"
          - "**/workbenches/testWorkbenchVariables.cy.ts"
          - "**/workbenches/workbenches.cy.ts"

    steps:

      - uses: actions/checkout@v4

      - uses: actions/checkout@v4
        with:
          repository: opendatahub-io/odh-dashboard
          ref: main
          path: odh-dashboard
          sparse-checkout: |
            frontend/

      # Otherwise we may end up with a random Python version?
      # https://github.com/jiridanek/rhoai-in-kind/issues/16
      - uses: actions/setup-python@v5
        id: setup-python
        with:
          python-version: '3.13'

      # ERROR: failed to create cluster: failed to pull image "docker.io/kindest/node:v1.31.6":
      #  command "docker pull docker.io/kindest/node:v1.31.6" failed with error: exit status 1
      - name: Pre-pull the kind image
        run: timeout 120s bash -c 'while ! docker pull kindest/node:v1.31.6; do sleep 1; done'

      - name: Kubernetes KinD Cluster
        id: kind-cluster
        uses: helm/kind-action@v1
        with:
          version: v0.27.0
          # Dashboard tests fail on k8s 1.32
          # https://hub.docker.com/r/kindest/node/tags
          node_image: "docker.io/kindest/node:v1.31.6"
          cluster_name: kind
          config: components/00-kind-cluster.yaml

      # âœ˜ Istiod encountered an error: failed to wait for resource: resources not ready after 5m0s: context deadline exceeded
      #    Deployment/istio-system/istiod (container failed to start: ImagePullBackOff: Back-off pulling image "docker.io/istio/pilot:1.25.1")Error: failed to install manifests: failed to wait for resource: resources not ready after 5m0s: context deadline exceeded
      #    Deployment/istio-system/istiod (container failed to start: ImagePullBackOff: Back-off pulling image "docker.io/istio/pilot:1.25.1")
      # CAUTION: the image tag is likely to go out of date, try to keep in sync
      - name: Pre-pull istio inside kind cluster
        run: |
          set -Eeuxo pipefail
          timeout 120s bash -c 'while ! docker pull docker.io/istio/pilot:1.25.1; do sleep 1; done'
          kind load docker-image docker.io/istio/pilot:1.25.1

      - name: Deploy stuff into Kubernetes
        run: ${PYTHON3} components/deploy.py --workbench-branch=${{ matrix.workbench_branch }}
        env:
          PYTHON3: "${{ steps.setup-python.outputs.python-path }}"

      - name: Print nbc logs
        if: "!cancelled()"
        run: |
          kubectl describe pods -n redhat-ods-applications -l app=notebook-controller
          kubectl logs -n redhat-ods-applications -l app=notebook-controller
      - name: Print odh nbc logs
        if: "!cancelled()"
        run: |
          kubectl describe pods -n redhat-ods-applications -l app=odh-notebook-controller
          kubectl logs -n redhat-ods-applications -l app=odh-notebook-controller

      # this is probably better done with Kyverno

      - name: Bring down Small workbench resource requests
        run: |
          components/patch-workbench-resource-requests.sh

      #- name: Install pnpm
      #  run: |
      #    # https://pnpm.io/installation#using-corepack
      #    corepack enable pnpm
      #    # https://github.com/pnpm/pnpm/issues/5344
      #    corepack use pnpm@latest
      #    corepack prepare pnpm@latest --activate
      #  working-directory: odh-dashboard/frontend

      # https://pnpm.io/continuous-integration#github-actions
      - name: Use Node.js with npm caching
        uses: actions/setup-node@v4
        with:
          cache: 'npm'
          cache-dependency-path: 'odh-dashboard/frontend/package-lock.json'

      - name: Setup cypress
        run: |
          set -Eeuxo pipefail

          # tput: No value for $TERM and no -T specified
          #sudo apt-get update
          #sudo apt-get install -y xterm ncurses-bin

          # sudo apt install -y libgtk-3-0 libgbm-dev libnotify-dev libnss3 libxss1 libxtst6 xauth xvfb
          npm install
        working-directory: odh-dashboard/frontend

      - name: Run cypress
        run: |
          set -Eeuxo pipefail
          # cypress run -b chrome --project src/__tests__/cypress --env skipTags=@Bug,grepTags=@Smoke --config video=true
          CY_TEST_CONFIG=${{ github.workspace }}/test-variables.yml npm run cypress:run -- --spec '${{ matrix.test_case }}'
        working-directory: odh-dashboard/frontend
        env:
          # dumb, xterm, xterm-256color
          TERM: xterm-256color

      # Error: The artifact name is not valid: cypress-videos-**/workbenches/testWorkbenchVariables.cy.ts. Contains the following character:  Asterisk *
      # Invalid characters include:
      #  Double quote ", Colon :, Less than <, Greater than >, Vertical bar |, Asterisk *, Question mark ?, Carriage return \r, Line feed \n, Backslash \, Forward slash /
      - name: Compute test-output name
        if: "!cancelled()"
        id: artifact-name
        shell: python {0}
        # language: python
        run: |
          import pathlib
          import os
          import re
          
          test_case = os.environ["TEST_CASE"]
          artifact_name = re.sub(r'^.*/(?P<filename>[^/]+)\.robot$', r'\g<filename>', test_case)
          forbidden_characters = "\"\\*/"
          table = str.maketrans(forbidden_characters, "_" * len(forbidden_characters))
          artifact_name = artifact_name.translate(table)
          pathlib.Path(os.environ["GITHUB_OUTPUT"]).write_text(f"artifact-name={artifact_name}")
        env:
          TEST_CASE: ${{ matrix.test_case }}

      - uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: cypress-report-${{ steps.artifact-name.outputs.artifact-name }}
          path: odh-dashboard/frontend/src/__tests__/cypress/results/e2e/index.html

      - uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: cypress-screenshots-${{ steps.artifact-name.outputs.artifact-name }}
          path: odh-dashboard/frontend/src/__tests__/cypress/results/e2e/screenshots

      - uses: actions/upload-artifact@v4
        if: "!cancelled()"
        with:
          name: cypress-videos-${{ steps.artifact-name.outputs.artifact-name }}
          path: odh-dashboard/frontend/src/__tests__/cypress/results/e2e/videos

      - name: Print nbc logs (again, at the end)
        if: "!cancelled()"
        run: |
          kubectl describe pods -n redhat-ods-applications -l app=notebook-controller
          kubectl logs -n redhat-ods-applications -l app=notebook-controller
      - name: Print odh nbc logs (again, at the end)
        if: "!cancelled()"
        run: |
          kubectl describe pods -n redhat-ods-applications -l app=odh-notebook-controller
          kubectl logs -n redhat-ods-applications -l app=odh-notebook-controller

      - name: Collect logs
        run: ${PYTHON3} components/logs.py
        id: collect-logs
        if: "!cancelled() && steps.kind-cluster.outcome == 'success'"
        env:
          PYTHON3: "${{ steps.setup-python.outputs.python-path }}"

      - name: Archive cluster-logs
        uses: actions/upload-artifact@v4
        if: "steps.collect-logs.outputs.debug_bundle_dir != '' && !cancelled()"
        with:
          name: cluster-logs-${{ steps.artifact-name.outputs.artifact-name }}
          path: ${{ steps.collect-logs.outputs.debug_bundle_dir }}